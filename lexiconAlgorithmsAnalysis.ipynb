{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nrclex import NRCLex\n",
    "import numpy as np\n",
    "import nltk\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# nltk.download('punkt')\n",
    "from textblob.classifiers import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------+-------------------+----------------------+\n",
      "| Text                                                                                                            |   Sentiment Score | Sentiment Category   |\n",
      "+=================================================================================================================+===================+======================+\n",
      "| I absolutely loved the new movie! The acting was superb, and the storyline kept me engaged from start to finish |                 8 | Joy                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------+-------------------+----------------------+\n",
      "| This product is terrible and a waste of money. I'm very disappointed.                                           |                -6 | Anger                |\n",
      "+-----------------------------------------------------------------------------------------------------------------+-------------------+----------------------+\n",
      "| The new google phone is okay, not great but not bad either.                                                     |                 0 | Neutral              |\n",
      "+-----------------------------------------------------------------------------------------------------------------+-------------------+----------------------+\n",
      "| New google buds are a no go. It's a complete failure.                                                           |                -3 | Negative             |\n",
      "+-----------------------------------------------------------------------------------------------------------------+-------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "texts = [\n",
    "    \"I absolutely loved the new movie! The acting was superb, and the storyline kept me engaged from start to finish\",\n",
    "    \"This product is terrible and a waste of money. I'm very disappointed.\",\n",
    "    \"The new google phone is okay, not great but not bad either.\",\n",
    "    \"New google buds are a no go. It's a complete failure.\"\n",
    "]\n",
    "\n",
    "def categorize_sentiment(score):\n",
    "    if score > 5:\n",
    "        return \"Joy\"\n",
    "    elif score > 0:\n",
    "        return \"Positive\"\n",
    "    elif score == 0:\n",
    "        return \"Neutral\"\n",
    "    elif score < -5:\n",
    "        return \"Anger\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "results = []\n",
    "for text in texts:\n",
    "    score = afinn.score(text)\n",
    "    category = categorize_sentiment(score)\n",
    "    results.append([text, score, category])\n",
    "\n",
    "headers = [\"Text\", \"Sentiment Score\", \"Sentiment Category\"]\n",
    "table = tabulate(results, headers, tablefmt=\"grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"new_df.csv\"\n",
    "df = pd.read_csv(file, sep=';', encoding=\"latin1\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Emotions Lexicon-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRCLEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions_NRCLex'] = df['comment'].apply(lambda x: NRCLex(x).affect_frequencies) \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(['emotions_NRCLex'], axis=1), df['emotions_NRCLex'].apply(pd.Series)], axis=1)\n",
    "df.head()\n",
    "# df.to_csv(\"NrclexLexicon.csv\", sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"NrclexLexicon.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_sentiment(sentiment):\n",
    "#     # Check if sentiment is a dictionary\n",
    "#     if isinstance(sentiment, dict):\n",
    "#         if sentiment.get('positive', 0) > sentiment.get('negative', 0):\n",
    "#             return 'Positive'\n",
    "#         elif sentiment.get('positive', 0) < sentiment.get('negative', 0):\n",
    "#             return 'Negative'\n",
    "#         else:\n",
    "#             return 'Neutral'\n",
    "   \n",
    "    \n",
    "    \n",
    "# df['sentiment'] = df['comment'].apply(get_sentiment)\n",
    "# display(df)\n",
    "# df.to_csv(\"NrclexLexicon.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the polarity and subjectivity\n",
    "def analyze_sentiment(comment):\n",
    "    blob = TextBlob(comment)\n",
    "    return pd.Series([blob.sentiment.polarity, blob.sentiment.subjectivity])\n",
    "\n",
    "# Function to label the sentiment based on polarity\n",
    "def label_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply the function to the comments column\n",
    "df[['polarity', 'subjectivity']] = df['comment'].apply(analyze_sentiment)\n",
    "\n",
    "# Apply the labeling function to the polarity column\n",
    "df['sentiment'] = df['polarity'].apply(label_sentiment)\n",
    "display(df)\n",
    "df.to_csv('TextBlobLexicon.csv', ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = np.array(df['comment'])\n",
    "# test_comments = comments[25000:]\n",
    "# sample_comments_ids = [1485, 7778, 12397,500, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the sample comments and calculate their sentiment polarity\n",
    "# for sample_id in sample_comments_ids:\n",
    "#     comment = test_comments[sample_id]\n",
    "#     print('Comment ID:', sample_id)\n",
    "#     print(\"Comment:\", comment)\n",
    "#     print('Predicted sentiment polarity:', textblob.TextBlob(comment).sentiment.polarity)\n",
    "#     print('*'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_polarity = [textblob.TextBlob(comment).sentiment.polarity for comment in test_comments]\n",
    "# predicted_sentiment = ['pozitiv' if score> 0.01 else 'negativ' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['sentiment'] = df['comment'].apply(lambda x: TextBlob(str(x)).sentiment.polarity if pd.notnull(x) else None)\n",
    "# df_sorted_descending= df.sort_values(by=\"sentiment\", ascending=False)\n",
    "# df.to_csv(\"TextBlobLexicon.csv\", sep=';')\n",
    "# display(df_sorted_descending)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFININ LEXICON ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "afn = Afinn()\n",
    "positive_threshold = 0.1\n",
    "negative_threshold = -0.1\n",
    "\n",
    "\n",
    "def calculate_sentiment(comment):\n",
    "    words = comment.split()\n",
    "    scores = [afn.score(word) for word in words if afn.score(word) != 0]\n",
    "    if scores:\n",
    "        return sum(scores) / len(scores)\n",
    "    else:\n",
    "        return 0\n",
    "df['sentiment_score'] = df['comment'].apply(calculate_sentiment)\n",
    "\n",
    "df['sentiment_score'] = df['sentiment_score'].apply(lambda x: '{:,.2f}'.format(x) if x != 0 else '0')\n",
    "\n",
    "def label_sentiment(score):\n",
    "    if score > positive_threshold:\n",
    "        return 'Positive'\n",
    "    elif score < negative_threshold:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(lambda x: label_sentiment(float(x.replace(',', ''))))\n",
    "\n",
    "\n",
    "output_file_with_labels = 'AfininLexicon.csv'\n",
    "df.to_csv(output_file_with_labels, index=False, sep=';')\n",
    "\n",
    "print(\"Sentiment-labeled comments saved to:\", output_file_with_labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_scores(sentence):\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    return sentiment_dict\n",
    "\n",
    "df['sentiment_dict'] = df['comment'].apply(sentiment_scores)\n",
    "df['neg'] = df['sentiment_dict'].apply(lambda x: x['neg'])\n",
    "df['neu'] = df['sentiment_dict'].apply(lambda x: x['neu'])\n",
    "df['pos'] = df['sentiment_dict'].apply(lambda x: x['pos'])\n",
    "df['compound'] = df['sentiment_dict'].apply(lambda x: x['compound'])\n",
    "\n",
    "df = df.drop(columns=['sentiment_dict'])\n",
    "\n",
    "\n",
    "def overall_sentiment(compound):\n",
    "    if compound >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['compound'].apply(overall_sentiment)\n",
    "\n",
    "df.to_csv('VaderLexicon.csv', index=False, sep=';')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparare indici de performanta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"lexicon_sentiments_labeld.csv\", delimiter=\";\")\n",
    "\n",
    "def calculate_accuracy(lexicon_column):\n",
    "    correct_predictions = (df['personal_sentiment'] == df[lexicon_column]).sum()\n",
    "    total_predictions = len(df)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "lexicon_columns = ['TextBlob_sentiment', 'Vader_sentiment', 'Afinin_Sentiment', 'NRCLex_sentiment']\n",
    "\n",
    "\n",
    "accuracy_scores = {}\n",
    "for lexicon_column in lexicon_columns:\n",
    "    accuracy_scores[lexicon_column] = calculate_accuracy(lexicon_column)\n",
    "\n",
    "\n",
    "print(\"Accuracy scores:\")\n",
    "for lexicon_column, accuracy in accuracy_scores.items():\n",
    "    print(f\"{lexicon_column}: {accuracy:.2%}\")\n",
    "\n",
    "\n",
    "best_lexicon = max(accuracy_scores, key=accuracy_scores.get)\n",
    "print(f\"\\nThe best performing lexicon is: {best_lexicon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pastel_palette = sns.color_palette(\"pastel\", len(lexicon_columns))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color=pastel_palette)\n",
    "plt.title('Accuracy of Lexicon Sentiment Analysis')\n",
    "plt.xlabel('Lexicon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1) \n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, \"{:.1%}\".format(yval), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_lexicon = max(accuracy_scores, key=accuracy_scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define classes\n",
    "classes = ['Positive', 'Negative', 'Neutral']\n",
    "\n",
    "# Calculate F1-score for each lexicon and sentiment class\n",
    "f1_scores = {}\n",
    "for lexicon_column in lexicon_columns:\n",
    "    lexicon_predictions = df[lexicon_column]\n",
    "    f1_scores[lexicon_column] = {}\n",
    "    for sentiment_class in classes:\n",
    "        true_labels = df['personal_sentiment'] == sentiment_class\n",
    "        predicted_labels = lexicon_predictions == sentiment_class\n",
    "        f1_scores[lexicon_column][sentiment_class] = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print F1-scores\n",
    "print(\"F1-scores:\")\n",
    "for lexicon_column, scores in f1_scores.items():\n",
    "    print(f\"{lexicon_column}:\")\n",
    "    for sentiment_class, f1 in scores.items():\n",
    "        print(f\"\\t{sentiment_class}: {f1:.2f}\")\n",
    "\n",
    "# Optionally, calculate weighted average F1-score\n",
    "weighted_f1_scores = {}\n",
    "for lexicon_column, scores in f1_scores.items():\n",
    "    weighted_f1_scores[lexicon_column] = sum(scores.values()) / len(scores)\n",
    "\n",
    "print(\"\\nWeighted average F1-scores:\")\n",
    "for lexicon_column, f1 in weighted_f1_scores.items():\n",
    "    print(f\"{lexicon_column}: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"new_df.csv\", delimiter=\";\")\n",
    "\n",
    "# Define lexicon columns and true sentiment labels\n",
    "lexicon_columns = ['TextBlob_sentiment', 'Vader_sentiment', 'Afinin_Sentiment', 'NRCLex_sentiment']\n",
    "true_labels = df['personal_sentiment']\n",
    "\n",
    "# Create confusion matrix for each lexicon\n",
    "for lexicon_column in lexicon_columns:\n",
    "    predicted_labels = df[lexicon_column]\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Positive\", \"Negative\", \"Neutral\"], yticklabels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'Confusion Matrix for {lexicon_column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING BAYIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
